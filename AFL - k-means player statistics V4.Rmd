---
title: "Determining AFL players field position by match day statistics - PCA & k means"
author: "Tom Perkins"
date: "16 January 2021"
output:
  pdf_document: default
  prettydoc::html_pretty:
    theme: caymen
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load dependencies, results='hide', message=FALSE, warning=FALSE}
# Data cleaning 
library(plyr)
library(snakecase)
library(tidyr)
library(dplyr)
library(readr)
library(reshape)
library(knitr)

# Data visualisation
library(corrplot)
library(taucharts) # not available for ANZ version of R 
library(purrr) # required for map_dbl
library(GGally)
library(ggplot2)
library(ggfortify)
library(plotly)
library(factoextra)
library(kableExtra)

# Data analysis 
library(ClustOfVar)
library(cluster)
```

```{r plot theme, results='hide', message=FALSE, warning=FALSE}
plot_theme <-
  theme(legend.position = "right",
        legend.direction = "vertical",
        legend.title = element_text(colour = "#4E4F4E",
                                    size = 8,
                                    face = "bold"),
        legend.text = element_text(colour = "#4E4F4E",
                                   size = 8),
        legend.key.height = grid::unit(0.6,"cm"),
        legend.key.width = grid::unit(0.6,"cm"),
        legend.margin = margin(0,0,0,0.2,"cm"), # move a little away from plot, to the right
        axis.text.x = element_text(size = 8,
                                   colour = "#4E4F4E"),
        axis.title.x = element_text(colour = "#4E4F4E",
                                    size = 9,
                                    face = "bold",
                                    margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(colour = "#4E4F4E",
                                    size = 9,
                                    face = "bold",
                                    margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.text.y = element_text(size = 8,
                                   vjust = 0.2,
                                   colour = "#4E4F4E"),
        axis.ticks = element_line(size = 0.2, 
                                  colour = "#878683"),
        panel.border = element_blank(),
        strip.text.x = element_text(size = 8, colour = "#6b6e6b"),
        strip.background = element_rect(fill="#fffdf2"),
        plot.margin = margin(0.4,0.4,0.4,0.4,"cm"),
        plot.title = element_text(colour = "#4E4F4E",
                                  hjust = 0,
                                  size = 10,
                                  face = "bold"),
        plot.subtitle = element_text(colour = "#6b6e6b",
                                     hjust = 0,
                                     size = 9),
        plot.caption = element_text(colour = "#4E4F4E",
                                    hjust = 0,
                                    vjust = 1,
                                    size = 6,
                                    face = "italic",
                                    margin = margin(-5,0,0,0)))

colour_theme <- c("#E4796A", "#E7B573", "#f5a253", "#638DCB")
```

## Introduction

This markdown provides an introduction to cluster analysis using principal components analysis & k-means. The goal is to evaluate if Australian Rules Football players field position can be determined by their match day statistics. Results of the cluster analysis will be evaluated against two different measures of field position - a [**ground truth**](https://datascience.stackexchange.com/questions/17839/what-is-ground-truth) to guide this analysis

The analysis is based upon a dataset I curated in a **["previous analysis"(https://perkot.github.io/afl-stats/)]**, & summarizes *average game-day statistics* such as average number of kicks, handballs & goals per game

This data was sourced from [AFLTables](https://afltables.com/afl/afl_index.html) & [Footywire](https://www.footywire.com/) - two of the best online resources for AFL statistics 

R users are fortunate that a wealth of data from these websites is accessible via the **["FitzRoy"](https://cran.r-project.org/web/packages/fitzRoy/index.html)** package

To begin, lets load in the data

```{r data load, results='hide', message=FALSE, warning=FALSE}
# player data .csv 
df.players <- read.csv("afl_player_statistics_pf.csv",
                       header = TRUE, 
                       stringsAsFactors = FALSE)
```

```{r print table preview, echo=FALSE}
# Print table 
kable(head(df.players)) %>% #head limits to just top rows
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", font_size = 8))
```

Each of the 950 rows reflects a unique player record

Columns 1:3 provide basic demographics (players name, team, games played). Column four is the ground truth for this dataset: field position

Columns five through 48 are *averaged* statistics (i.e. kicks, handballs, spoils, tacklers per game). These are the columns we will choose from to perform the k-means cluster analysis 

## Data preparation

There are two initial steps which form important assumptions of the analysis 

Firstly, I have removed any player to take part in *less* than 22 games (equivalent to a single full-season) 

My reasoning was that players with a small number of games may skew the data. Younger, inexperienced players with a small sample of games will likely have limited output, not typically reflective of the position that they play. Similarly, players with a small number of games due to injury may generate numbers otherwise not reflective of their typical output over a larger sample size

```{r minimum games, message=FALSE, warning=FALSE}
# filter to only players who have played at least an entire season worth of games 
df.players <- df.players %>%
  dplyr::filter(Games >= 22)
```

Secondly, I have *scaled* the data. 

Scaling is a recommended step for k-means, & a mandatory step for principal components analysis 

If a dataset includes measures on different scales, or at a minimum has variables with different variances, it risks the analysis being biased in favor of variables with *large* differences 

The present data clearly has different scales. For example, *metres gained* is an averaged measure of distance, *disposal efficiency* is expressed as an averaged percentage, whilst *goals* is an averaged count  

This data also contains metrics with different variances. The range of values for *goals* spans from an average of zero to 2.96 per game, whilst *metres gained* spans from an averaged low of 28m, to a high of 484m per game 

By using **scale()**, & passing the arguments *center = TRUE* & *scale = TRUE*, the data is converted into **z-scores** - an expression of the number of standard deviation units by which a value is above or below the mean of the measurement in question

z-scores improve multivariate analysis by reducing bias (aka. error) attributable to differences in magnitude between variables

```{r scale, message=FALSE, warning=FALSE}
# scale all of our variables into z-scores
  # center = TRUE : remove the mean
  # scale = TRUE : divide by SD
df.players.scaled <- as.data.frame(scale(df.players[,5:48],
                                         center = TRUE, 
                                         scale = TRUE))
```

## Data model 

As mentioned, this analysis is largely exploratory. Any combination of the 44 match-day metrics could be included to perform a k-means analysis 

However, k-means requires due consideration of what variables are included in the model. A dump of all variables will likely include ineffective variables, which may *negatively impact* on the result by obscuring useful variance

Identifying a satisfactory combination of variables is critical to success. This makes clustering an iterative & patient process, that is aided by *subject matter-expertise*. Even then, some degree of trial-&-error is required : adding & subtracting variables, & documenting the subsequent effect on the output 

This analysis relied upon intuition around what match-day statistics would likely discriminate players, then trial & error of different combinations

Some metrics were conspicuous (i.e. *hitouts* as something likely to distinguish players), whilst others were more ambiguous (*uncontested possessions* seemed less clear in its discriminatory efficacy)

The 15 statistics included in the model are summarized below

```{r scale1, message=FALSE, warning=FALSE}
set.seed(123)
```

```{r Model 1 (1), message=FALSE, warning=FALSE}
M1 <- df.players.scaled %>%
  select(
    hitouts,
    hitouts_to_advantage,
    ruck_contests,
    score_launches,
    clearances,
    contested_possessions,
    uncontested_possessions,
    centre_clearances,
    stoppage_clearances,
    goals,
    tackles_inside_fifty,
    f50_ground_ball_gets,
    rebounds,
    contest_def_one_on_ones,
    disposal_efficiency_percentage
    )
```

With our data finalized we can jump into the first-step: **principal components analysis**

## Principal Components Analysis 

Principal Component Analysis (PCA) is a common technique applied *prior* to cluster analysis, but a valuable analysis in its own right. It is a method for **dimensionality-reduction** / **compression** of a dataset. It transforms a "larger set" of variables into a smaller set, that retains *most* of the variance of the larger set in a smaller number of components

Although some information will be lost in compression, the idea is that a small degradation in accuracy is traded-off for an increase in parsimony - a statistical equivalent of a compressed MP3 retaining much of the quality of an uncompressed WAV file, at a fraction of the file-size

Thus, *Principal components* are new variables, derived mathematically from a linear combination of the initial variables. 

To paraphrase [this awesome guide](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)

*Principal components are calculated, such that most of the information/variance is compressed into the first components. In other words, a data-set with 10 variables will produce 10 principal components, but the derivation of principal components will maximize the amount of variance into the first component, then the second and so on. In effect, this process of dimensional reduction will allow a similar amount of variance from a 10 variable model to be extrapolated from a smaller number of principal components* 

As mentioned, standardization is critical for PCA. The reason being that it is quite sensitive to the variances of the initial variables

PCA can be [beneficial for noise reduction](https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca#:~:text=clustering%20pca%20k%2Dmeans,in%20practice%20(noise%20reduction).), although this benefit is tenuous & dependent on the data 

PCA does have one very obvious benefit to cluster analysis - it allows for simpler visualization of results. It is nigh on impossible to provide a simple summary visualization of higher-dimensional data. As such, dimensional reduction allows for more information from the analysis to be visualized in a two-dimensional format. Later, I will combine results of PCA & k-means into single visualizations 

### Covariance Matrix Computation

Key to the concept of dimensional reduction is the covariance matrix, which summarizes the relationship between variables included in the PCA. It is likely that some variables will share covariance with one another, in such a way that they contain redundant information

Note here that covariance is distinct from correlation - related, but different metrics

Correlation values are *standardized* (range between -1 and 1) & measure *relationship strength* & *direction*. 

Covariance values are *non-standardised*, & measure *only direction*. [Click here for a more in depth examination of this topic](https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22#:~:text=between%20two%20variables.-,%E2%80%9CCovariance%E2%80%9D%20indicates%20the%20direction%20of%20the%20linear%20relationship%20between%20variables,a%20function%20of%20the%20covariance.) The role of covariance, a non-standardized metric, in derivation of principal components is why scaling is essential prior to analysis  

### Eigenvectors & Eigenvalues

Principal components are constructed sequentially

The first principal component is mathematically derived to: 

(1) **Maximize variance** - i.e. the highest spread of values along line of best fit - calculated as an average of the squared distances 
(2) **Minimize Error** - i.e. the shortest distance of data points from their position on the line of best fit, calculated as an average of the squared distances from the projected points

The sum of *variance* & *error* is constant, so the statistical benefit of PCA is a best fitting line that both maximizes variance & minimizes error

**Eigenvectors** are the **directions** of the axes that possess the most variance, and the **eigenvalues** are a numerical indicator (coefficient) expressing the **amount** of variance in each principal component. The smallest eigenvalues correspond to the directions with the least variation

The second principal component is then calculated in precisely the same way, with the condition that it is *orthogonal* to the first principal component. In other words, in this rotated factor space, it is at 90 degrees, & completely uncorrelated with the first principal component. This process is completed for as many principal components as there are variables in the data-set

[This link](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) provides a more in depth guide to principal components, alongside some amazing two-dimensional animations illustrating the concept of maximizing variance & minimizing error

The mathematics of PCA is quite complex, but the programming is very straight-forward & can be completed in a line-of-code  

```{r PCA (1), message=FALSE, warning=FALSE}
# PCA Model
M1.pca <- prcomp(M1)
```

For illustrative purposes, I have calculated the eigenvalue for the first principal component (*square of the standard deviation of the principal component*)

By dividing the first eigenvalue by the sum of all eigenvalues, we can determine the percentage of variance explained - **37.43%**

```{r PCA (2), message=FALSE, warning=FALSE}
# Calculate Eigenvalues
M1.ev <- M1.pca$sdev^2 
# Calculate variance explained by the first principal component 
(M1.ev[1] / sum(M1.ev))*100
```

Fortunately, passing our PCA to the function *"summary"* will auto-calculate the standard deviation, total variance, & cumulative variance explained for each principal component

This is an especially useful, it shows 85% of variance in this dataset can be explained by the first three principal components - not bad! 

```{r PCA (2), message=FALSE, warning=FALSE}
summary(M1.pca)
```

The variance explained per principal component can be visualized with a **scree plot** - a simple bar-chart representing how many principal components are required before variance explained drops off 

Corroborating the raw numbers, we can clearly see that **three** principal components in this data-set is the magic number 

```{r PCA (3), message=FALSE, warning=FALSE}
fviz_eig(M1.pca)
```

Here we reach the punchline of PCA, & the value of dimension-reduction. We can explain 85% of the variance of 15 match-day metrics With just three principal components

An obvious limitation of principal components is that they are less easy to interpret than raw metrics Fortunately, we can explore the variable **loadings** of the principal components. [Loadings are the covariances/correlations between the original variables and the unit-scaled components](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another)

By extracting variable loadings we generate a dataframe where each column represents a principal component, & each row represents the original variables correlation with each principal components [Datacamp](https://www.datacamp.com/community/tutorials/pca-analysis-r) has a nice tutorial explaining this further

```{r PCA (4), message=FALSE, warning=FALSE}
M1.pca.df <- data.frame(M1.pca$rotation)
```

```{r PCA (4B), echo=FALSE}
# Print table 
kable(M1.pca.df) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", font_size = 8))
```

For the first three principal components, we can crudely summarize which variables demonstrated the strongest association to each principal component 

**PC 1** - Clearances / Contested possessions
**PC 2** - Score Launches / Hitouts to Advantage
**PC 3** - Uncontested Possessions / contested possessions / Rebounds

Further, we can visualize **all** of the variable loadings for the *first two* principal components. Each point on the plot reflects an individual player. The x-axis depicts PC1, whilst the y-axis depicts PC2

```{r PCA (5), message=FALSE, warning=FALSE}
p1 <- 
autoplot(M1.pca, data = M1,
         loadings = TRUE, 
         loadings.colour = '#36332e',
         loadings.size = 0.5,
         loadings.label.colour = '#36332e',
         loadings.label = TRUE,
         loadings.label.size = 2,
         loadings.label.repel = T
         ) +
  scale_x_continuous(name = "Principal Component 1 (37.4%)") +
  scale_y_continuous(name = "Principal Component 2 (27.3%)") +
  geom_point(colour = "#E7B573") +
  theme_minimal() +
  plot_theme 

p1$layers[[2]]$aes_params$size <- 0.1 # change line thickness 
p1
```

Principal component [1] is depicted along the x-axis, while principal component [2] is depicted along the y-axis. 

The strength of each variables contribution to these two components is depicted by the arrowed-lines. The strongest variables related to component [1] are pulled furthest to the right, while the strongest variables related to component [2] are pulled furthest to the top of the plot 

It is notable the variable loadings appear to *clump* into four distinct patterns. Immediately to any fan of AFL, this should be familiar. At face value, they look to be aligning closely with our *ground truth* - field position

**Ruck-centric** statistics at one o-clock (hit-outs, ruck contests, score-launches)
**Midfield-centric** statistics at three o-clock (clearances, contested possessions)
**Forward-centric** statistics at five o'clock (goals, tackles inside 50)
**Defense-centric** statistics at ten o'clock (rebounds, defensive one-on-one contests)

```{r PCA (6), message=FALSE, warning=FALSE}
# coordinates for each player for first 3 principal components into dataframe
M1.pca.pc3 <- data.frame(M1.pca$x[,1:3]) 
```

## k-means 

**"Clustering"** is a technique used to explore sub-groups of observations within a data set. Observations are grouped in such a way that they are more similar to one another (by some statistical criteria), than to observations assigned to other groups

**k-means clustering** is probably the most common algorithm for partitioning 

k-means 'groups' observations (in this case players) to their nearest mean. In this case *nearest* denotes the mean with the least squared *Euclidean distance*. Means in k-means analysis are calculated as **centroids** - the arithmetic average position of all of the points in the cluster. Clusters are defined such that the total **within-cluster sum of square (WSS)** is minimized

**'k'** represents the number of clusters the data will be separated into. Determining k is an important *a priori* decision for the analysis

k-means is fast & powerful, but comes with caveats 

[1] k-means will partition data, even if no real clusters exist. *Real* is a vague term here, but the point is that even if a data-set amounts to random noise, the k-means algorithm *will not fail*. Clusters will be generated even if they are meaningless. Fortunately there are ways to evaluate *cluster quality*

[2] k-means generates ~ spherical clusters. As such, visual inspection of k-means results is critical. As emphasized by this [awesome analysis by David Robinson](http://varianceexplained.org/r/kmeans-free-lunch/), k-means can successfully reduce WSS, but miss obvious clusters in the dataset should they not be spherical. As such it is horses for courses - finding the right clustering algorithm that is able to service the assumptions of the dataset

For more information on k-means/principal components analysis, [this towards data science](https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff) article is an easily digestible summary 

As mentioned, the number for *k* is a decision for the analyst. Generally, the value for k is best determined by corroborating 

(a) **knowledge around the data** - ground truths or known groupings & 
(b) **optimization methods** -  to determine which value of k best minimizes WSS

As mentioned we have a ground truth for this analysis (field position). However, field position can be divided into broad categories, or more granular sub-categories. As such, optimization methods are still valuable to quantify the relationship between game day statistics and how players can be grouped

Below I have applied two of the most popular optimization methods for determining k

### Elbow Method

In a k-means analysis, increasing the value of k will reduce WSS (for the most part). You could imagine drawing a line-chart with the *n* of *k* along the x-axis, & total WSS along the y-axis. As k increases, the line depicting WSS reduces toward zero

The elbow method does just this. It is a visual heuristic, to locate the point along this line at which adding further k has *minimal impact* upon the WSS. Literally, the *elbow* of the line

This method relies upon running a k-means analysis, for different values of k, & observing the WSS 

```{r k means (1), message=FALSE, warning=FALSE}
wss <- (nrow(M1)-1) * sum(apply(M1, 2, var)) # calculate wss
for (i in 1:15) wss[i] <- sum(kmeans(M1, centers = i)$withinss) # for k values 1:15
wss.df <- as.data.frame(wss) # convert to df
```

```{r k means (2), message=FALSE, warning=FALSE}
wss.df %>% 
  ggplot(aes(y=wss, x=1:15)) +
  geom_line(colour="#E4796A") +
  geom_point(colour="#E4796A") +
  scale_x_continuous(
    breaks=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15),
    labels=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15)) +
  scale_y_continuous(
    breaks=c(0,2000,4000,6000,8000,1000),
    labels=c(0,2000,4000,6000,8000,1000)) +
  labs(x = "k",
       y = "within cluster sum of squares",
       title = "Elbow plot",
       subtitle = "Depicting within cluster sum of squares against number of clusters") +
  theme_minimal() +
  plot_theme 
```

Inspecting the plot above, the point of the elbow should hopefully be obvious. We can clearly see an *elbow* at k = 4, making four clusters the *suggested* number, according to this method

### Silhouette Method 

Silhouette's are a nice compliment to elbow plots. Rather than measuring WSS, they are an indicator as to the *quality* of clustering. The optimal number of clusters is the one that maximize the average silhouette over a range of possible values for k

The silhouette ranges from −1 to +1, with a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters

```{r k means (2), message=FALSE, warning=FALSE}
# function to compute average silhouette for k clusters
M1.sil <- function(k) 
{
  km.res <- kmeans(M1, 
                   centers = k, 
                   nstart = 25)
  ss <- silhouette(km.res$cluster, 
                   dist(M1))
  mean(ss[, 3])
}
```

```{r k means (3), message=FALSE, warning=FALSE}
# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15
# extract avg silhouette for 2-15 clusters
M1_sil_values <- map_dbl(k.values, M1.sil)
sil.df <- as.data.frame(M1_sil_values)
```

```{r k means (4), message=FALSE, warning=FALSE}
sil.df %>% 
  ggplot(aes(y=M1_sil_values, x=2:15)) +
  geom_line(colour="#E4796A") +
  geom_point(colour="#E4796A") +
  scale_x_continuous(
    breaks=c(2,3,4,5,6,7,8,9,10,11,12,13,14,15),
    labels=c(2,3,4,5,6,7,8,9,10,11,12,13,14,15)) +
  scale_y_continuous(
    breaks=c(0.1,0.2,0.3,0.4,0.5),
    labels=c(0.1,0.2,0.3,0.4,0.5)) +
  labs(x = "k",
       y = "average silhouettes",
       title = "Silhouette plot",
       subtitle = "Depicting average silhouettes against number of clusters") +
  theme_minimal() +
  plot_theme
```

The plot above tells us two useful things

Firstly, that k = 4 is very clearly the optimal division of clusters for this dataset. It's nice that the Silhouette & Elbow plots both yield the same suggested k, providing consistent corroborating evidence for this cluster number 

Secondly, it also quantifies with numerical indicator the quality of the clustering. The average silhouette for k = 4 was **0,46**

An interpretation [offered here of silhouette width averages](https://web.archive.org/web/20111002220803/http://www.unesco.org:80/webworld/idams/advguide/Chapt7_1_1.htm) suggests that a silhouette average score between 0.26 & 0.50 is indicative of a *weak* & *potentially artificial* cluster structure

This on the surface is disappointing, but again, needs to be interpreted within the context of what we know about this data. We can concede at a minimum, the clustering results are not strong. But this isn't wholly surprising when considering the highly dynamic nature of AFL football, & immense diversity of athletes & outputs of players.  
It would be premature to outright dismiss the results as *artificial*. This is where subject matter expertise becomes important to the analysis, as we do know that there exist real-life groupings for players

### Analysis

With a **k value** established, running k-means analysis is straight forward. We will specify two additional arguments 

**nstart = 25** : The number of times the initial starting points are re-sampled. It looks for the initial starting points that have the lowest WSS

**iter.max = 1000** : the number of times the algorithm will repeat the cluster assignment and moving of centroids

```{r k means (5), message=FALSE, warning=FALSE}
# Elbow & Silhouette plot indicate optimal k = 4
# Scree plot demonstrated 85% of variance could be explained by first 3 components
k <- kmeans(M1.pca.pc3, 4, 
            nstart = 25, 
            iter.max = 1000)
k
```

Running the analysis produces several outputs 

1) *Cluster means* for each cluster (k), & each principal component included in the analysis 

2) *Cluster assignment* (either group 1, 2, 3 or 4) for each included player

3) Statistics related to *variance explained*. **Total sum of squares** is the sum of squared distances of each observation to the overall sample mean. Sample means are also calculated per cluster. Thus, the sum of squared distances of each of the four cluster mean to the overall mean is the **between group sum of squares** 

The **78.5%** represents the total percentage of variance explained by the clusters. Generally a larger percentage is favorable, but it is also directly related to how many clusters are included. Increasing k will result in an increase to this percentage. If there were as many clusters as there were observations (i.e. k = 554), then this value would be 100% as each individual represents their own group. This would also be completely self-defeating

Let's now visualize the results of k-means in three ways:

### Clusters

```{r k means plot (1), message=FALSE, warning=FALSE}
plot.data <- fviz_cluster(k, data = M1) # save to access $data
data <- plot.data$data # this is all you need
hull_data <- data %>%
  group_by(cluster) %>%
  slice(chull(x, y))
ggplot(data, aes(x, y)) + 
  geom_point(shape = 1) +
  geom_polygon(data = hull_data, 
               alpha = 0.5, 
               aes(fill = cluster, linetype = cluster)) +
  scale_colour_manual(values = colour_theme) +
  scale_fill_manual(values = colour_theme) +
  scale_x_continuous(name = "Principal Component 1 (37.4%)") +
  scale_y_continuous(name = "Principal Component 2 (27.3%)") +
  geom_point(alpha = 0.01) +
    labs(title = "Cluster Plot",
       subtitle = "Depicting k = 4 for first two principal components") +
  theme_minimal() +
  plot_theme
```

Similarly to the PCA plot, each player is depicted against the first two principal components in this analysis (mindful three were used to generate the results). The *clusters* to emanate from the k-means analysis are shown by the polygonal shapes overlaying the data-points. This shows some *clumping* of the data, but not clear separation. There is more a gradient-like effect of players overlapping, but pushing in different directions. Were this to be a spectacularly successful cluster analysis, groups would not overlap, occupying their own space in the visualization. As identified earlier, this is probably too taller ask for inherently heterogeneous data such as as AFL player statistics 

### Clusters overlayed with PCA variable loadings

```{r k means plot (2A), message=FALSE, warning=FALSE}
# join cluster groupings back to data
M1[,"Cluster"] <- k$clust
# coerce to factor 
M1$Cluster <- as.factor(M1$Cluster)
# plot now with k means clusters
p1 <-
autoplot(M1.pca, 
         data = M1,
         colour = "Cluster", # need to join k$clust back to M1
         loadings = TRUE, 
         loadings.colour = '#666257',
         loadings.size = 0.5,
         loadings.label.colour = '#666257',
         loadings.label = TRUE,
         loadings.label.size = 2,
         loadings.label.repel = T
         ) +
  scale_colour_manual(values = colour_theme) +
  scale_x_continuous(name = "Principal Component 1 (37.4%)") +
  scale_y_continuous(name = "Principal Component 2 (27.3%)") +
  geom_point(alpha = 0.01) +
      labs(title = "Cluster Plot",
      subtitle = "Depicting k = 4 for first two principal components + variable loadings") +
  theme_minimal() +
  plot_theme
p1$layers[[2]]$aes_params$size <- 0.1 # change line thickness 
p1
```

The above plot combines components of the PCA & k-means analysis. The colour of each data-point represents the k-means cluster it has been assigned to, whilst the gray lines represent the variable loadings for the first two principal components. This shows fairly clearly that each of the four clusters align pretty closely with metrics one would expect to be associated with a players field position 

### Clusters overlayed with PCA variable loadings & player labels

With 500 + data-points the plot is too dense to include every single player. As such I have leveraged data from [stats insider](https://www.statsinsider.com.au/afl/player-ratings) that includes player ratings, to limit player labels to only the top handful of players in each position 

The player positions listed on stats insider are more granular

* Midfielder
* Ruck
* Key Forward
* General Forward
* Key Defender
* General Defender

Resulting in 30 player labels included 

```{r k means evaluate3, message=FALSE, warning=FALSE}
# retrieve all player names for labelling 
M1.pca.pc3[,"Player"] <- df.players$Player_Name
# retrieve only top players names for labelling 
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Harris Andrews"] <- "Harris Andrews"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Steven May"] <- "Steven May"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Darcy Moore"] <- "Darcy Moore"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Jacob Weitering"] <- "Jacob Weitering"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Jeremy McGovern"] <- "Jeremy McGovern"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Jake Lloyd"] <- "Jake Lloyd"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Caleb Daniel"] <- "Caleb Daniel"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Rory Laird"] <- "Rory Laird"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Daniel Rich"] <- "Daniel Rich"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Bachar Houli"] <- "Bachar Houli"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Toby Greene"] <- "Toby Greene"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Chad Wingard"] <- "Chad Wingard"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Shai Bolton"] <- "Shai Bolton"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Isaac Heeney"] <- "Isaac Heeney"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Liam Ryan"] <- "Liam Ryan"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Tom Hawkins"] <- "Tom Hawkins"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==   "Lance Franklin"] <-  "Lance Franklin"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Josh J. Kennedy"] <- "Josh J. Kennedy"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Jack Gunston"] <- "Jack Gunston"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Lachie Neale"] <- "Lachie Neale"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Dustin Martin"] <- "Dustin Martin"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Travis Boak"] <- "Travis Boak"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Jack Steele"] <- "Jack Steele"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Nat Fyfe"] <- "Nat Fyfe"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Max Gawn"] <- "Max Gawn"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Nic Naitanui"] <- "Nic Naitanui"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Todd Goldstein"] <- "Todd Goldstein"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==  "Rowan Marshall"] <- "Rowan Marshall"
M1.pca.pc3$Player_Top[M1.pca.pc3$Player ==   "Jarrod Witts"] <-  "Jarrod Witts"
```

```{r k means plot (2B), message=FALSE, warning=FALSE}
p2 <-
autoplot(M1.pca, 
         data = M1,
         colour = "Cluster",
         ) +
  scale_colour_manual(values = colour_theme) +
  scale_x_continuous(name = "Principal Component 1 (37.4%)") +
  scale_y_continuous(name = "Principal Component 2 (27.3%)") +
  geom_point(alpha = 0.01) +
    geom_text(vjust=-1, 
              label=M1.pca.pc3$Player_Top,
              size = 2,
              colour = "#4E4F4E") +
      labs(title = "Cluster Plot",
      subtitle = "Depicting k = 4 for first two principal components + variable loadings") +
  theme_minimal() +
  plot_theme
p2$layers[[2]]$aes_params$size <- 0.1 # change line thickness 
p2
```

You can see quite clearly that greater cluster separation occurs when limiting to star players. Presumably they are exemplars of their field-position, & thus provide a clearer profile for grouping

### Model Evaluation

#### Ground Truth 

As alluded to, we have a pre-existing way of grouping players - their **field position**. In Australian rules, there are four over-arching positions (midfielders, rucks, defenders & forwards). This provides some encouraging validation of the the Elbow & Silhouette Plots which both indicated k = 4 clusters 

Player position is uncontroversial, but exactly how it is measured / quantified is a little trickier. Complicating factors include:

* players can play in *multiple* positions (~ 15% estimated to be dual-position players)
* players can *change position* over the course of their career 
* positions can be defined with more granularity (i.e. key forwards versus small forwards)

So although the ground truth in this context provides a compass of sorts for understanding the k-means results, it isn't an immutable truth universal to all circumstances, or observations 

We will proceed with two methods for determining a players field position

**[1]** The most frequent part of the ground a player is listed on the team sheet

**[2]** Assigned position, sourced & defined by *Stats Insider*

#### Player Position - team sheet

We can quantify exactly how many players cluster group matched their most commonly named starting position:

```{r k means plot (3), message=FALSE, warning=FALSE}
# join cluster groupings back to data
M1[,"Position"] <- df.players$PositionType
# coerce to factor 
M1$Position <- as.factor(M1$Position)
# plot now with k means clusters
p3 <-
autoplot(M1.pca, 
         data = M1,
         colour = "Position", # need to join k$clust back to M1
         loadings = TRUE, 
         loadings.colour = '#666257',
         loadings.size = 0.5,
         loadings.label.colour = '#666257',
         loadings.label = TRUE,
         loadings.label.size = 2,
         loadings.label.repel = T
         ) +
  scale_colour_manual(values = colour_theme) +
  scale_x_continuous(name = "Principal Component 1 (37.4%)") +
  scale_y_continuous(name = "Principal Component 2 (27.3%)") +
  geom_point(alpha = 0.01) +
      labs(title = "Cluster Plot",
      subtitle = "Depicting player position for first two principal components + variable loadings") +
  theme_minimal() +
  plot_theme
p3$layers[[2]]$aes_params$size <- 0.1 # change line thickness 
p3
```

```{r k means evaluate, message=FALSE, warning=FALSE}
# join players back
M1[,"Player"] <- df.players$Player_Name
# as character
M1$Cluster <- as.character(M1$Cluster)
# Change cluster names to ~ position
M1$Cluster[M1$Cluster == "1"] <- "Defender"
M1$Cluster[M1$Cluster == "2"] <- "Forward"
M1$Cluster[M1$Cluster == "3"] <- "Midfield"
M1$Cluster[M1$Cluster == "4"] <- "Ruck"
# look for mismatches 
M1$PSP.Match <- M1$Cluster == M1$Position
```

```{r k means evaluate2, message=FALSE, warning=FALSE}
M1 %>%
  group_by(PSP.Match) %>%
  tally() %>% 
  mutate(percent = n / sum(n)*100)
```
~ **84%** of players cluster/position matched

Looking more closely at the 87 players to mismatch:

```{r k means evaluate3, message=FALSE, warning=FALSE}
mismatch_position.s <- M1 %>% 
  filter(PSP.Match == FALSE) %>% # only mismatches
  ddply(.(Cluster, Position), nrow) %>% # count combinations
  arrange(desc(V1)) # order descending
mismatch_position.s
```

The majority of mismatches (59/87, 67%) were players whose starting position was as a midfielder, but the cluster defined them as either a defender or forward

Examining these on a case-by-case basis, many of these players position is ambiguous to begin with. For example, the cluster algorithm designated Dustin Martin as a midfielder, while the *ground truth*, based on his named position in the team was as a forward. This edge case is perhaps revealing of a limitation of this analysis. Beholding players positions to the broad categories of midfielder, defender, forward, or ruck may be too limiting. Dustin Martin is probably better captured as a *utility*, or a hybrid *midfield-forward*

Even so inspection of the mismatches does reveal some players for which the cluster algorithm would appear to have got entirely *wrong* (at least wrong in so far as the clusters are a representation of ground-position!). For example, Tom Scully is not known to play forward in any capacity, typically occupying the wings as a midfielder.

#### Player Position - Stats Insider

Popular sporting website Stats Insider provided another simple online resource, that included the field position for AFl players. This comes from their previously referenced [*AFL player ratings*](https://www.statsinsider.com.au/blog/afl/explaining-the-stats-insider-afl-player-ratings)

```{r k means plot (4) pre-work, message=FALSE, warning=FALSE}
positions.SI <- read.csv("afl_stats_position_stats_insider.csv")
```

Stats Insider provides a more granular definition of field position, so for the purposes of comparison to the present model, I have rolled them up to there *parent* position

```{r k means plot (4) pre-work2, message=FALSE, warning=FALSE}
# Re-group to 4 position types 
positions.SI$PositionSI[positions.SI$Position ==   "Gen Def"] <-  "Defender"
positions.SI$PositionSI[positions.SI$Position ==   "Key Def"] <-  "Defender"
positions.SI$PositionSI[positions.SI$Position ==   "Gen Fwd"] <-  "Forward"
positions.SI$PositionSI[positions.SI$Position ==   "Key Fwd"] <-  "Forward"
positions.SI$PositionSI[positions.SI$Position ==   "Mid"] <-  "Midfield"
positions.SI$PositionSI[positions.SI$Position ==   "Ruck"] <-  "Ruck"
```

As per previously, we can directly compare field position to the models assigned grouping

```{r}
# limit df to two columns of interest
positions.SI <- positions.SI %>% 
  select(Player, PositionSI)
# join stats insider positions to df 
M1 <- left_join(M1, positions.SI, by = "Player")
```

```{r}
# a couple of discrepant players - fill in with known position type 
M1$PositionSI <- ifelse(is.na(M1$PositionSI), M1$Position, M1$PositionSI)
```

```{r}
# Change these so their cluster name is not numeric 
M1$PositionSI[M1$PositionSI == "1"] <- "Defender"
M1$PositionSI[M1$PositionSI == "2"] <- "Forward"
M1$PositionSI[M1$PositionSI == "3"] <- "Midfield"
M1$PositionSI[M1$PositionSI == "4"] <- "Ruck"
```

```{r k means evaluate16, message=FALSE, warning=FALSE}
# look for mismatches 
M1$SI.Match <- M1$Cluster == M1$PositionSI
```

```{r k means evaluate17, message=FALSE, warning=FALSE}
M1 %>%
  group_by(SI.Match) %>%
  tally() %>% 
  mutate(percent = n / sum(n)*100)
```

```{r k means evaluate3, message=FALSE, warning=FALSE}
mismatch_position.si <- M1 %>% 
  filter(SI.Match == FALSE) %>% # only mismatches
  ddply(.(Cluster, Position), nrow) %>% # count combinations
  arrange(desc(V1)) # order descending
mismatch_position.si
```

Ben McEvoy
Bryce Gibbs
Jack Watts

Career outputs versus current position as of season

```{r k means plot (4), message=FALSE, warning=FALSE}
# coerce to factor 
M1$PositionSI <- as.factor(M1$PositionSI)

M1.pca.pc3$Player_Diff[M1.pca.pc3$Player ==  "Ben McEvoy"] <- "Ben McEvoy"
M1.pca.pc3$Player_Diff[M1.pca.pc3$Player ==  "Bryce Gibbs"] <- "Bryce Gibbs"
M1.pca.pc3$Player_Diff[M1.pca.pc3$Player ==  "Jack Watts"] <- "Jack Watts"

# plot now with k means clusters
p4 <-
autoplot(M1.pca, 
         data = M1,
         colour = "PositionSI", # need to join k$clust back to M1
         loadings = TRUE, 
         loadings.colour = '#666257',
         loadings.size = 0.5
         # loadings.label.colour = '#666257',
         # loadings.label = TRUE,
         # loadings.label.size = 2,
         # loadings.label.repel = T
         ) +
  scale_colour_manual(values = colour_theme) +
  scale_x_continuous(name = "Principal Component 1 (37.4%)") +
  scale_y_continuous(name = "Principal Component 2 (27.3%)") +
  geom_point(alpha = 0.01) +
  geom_text(vjust=-1, 
          label=M1.pca.pc3$Player_Diff,
          size = 2,
          colour = "#4E4F4E") +
      labs(title = "Cluster Plot",
      subtitle = "Depicting player position for first two principal components + variable loadings") +
  theme_minimal() +
  plot_theme
p4$layers[[2]]$aes_params$size <- 0.1 # change line thickness 
p4
```


## Conclusion 






















& can be determined in two ways

**Internal indices** - to measure the goodness of a clustering structure without external information (Tseng et al., 2005). In this case, relying upon statistical outputs such as elbow plots / silhouette plots

**External indices** - to evaluate the results of a clustering algorithm based 
on a known cluster structure of a data set. In other words, already known, real-world groupings










