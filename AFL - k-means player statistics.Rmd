---
title: "Using k-means to cluster AFL players based on their average match day statistics"
author: "Tom Perkins"
date: "16 January 2021"
output:
  prettydoc::html_pretty:
    theme: caymen
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

General notes

the performance of k-means algorithm tends to be affected by skewed data distributions, i.e., imbalanced data. They often produce clusters of relatively uniform sizes, even if input data have varied cluster size, which is called the “uniform effect.”
# https://link.springer.com/chapter/10.1007/978-3-319-13731-5_54

Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. 

An additional disadvantage of K-means is that it’s sensitive to outliers and different results can occur if you change the ordering of your data. The Partitioning Around Medoids (PAM) clustering approach is less sensititive to outliers and provides a robust alternative to k-means
https://www.datanovia.com/en/lessons/k-medoids-in-r-algorithm-and-practical-examples/

Evaluate feature importance
https://cran.r-project.org/web/packages/FeatureImpCluster/readme/README.html

Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.

k means assumptions
  k-means assume the variance of the distribution of each attribute (variable) is     spherical;

  all variables have the same variance;

  the prior probability for all k clusters are the same, i.e. each cluster has         roughly equal number of observations; If any one of these 3 assumptions is           violated, then k-means will fail.

## Premise

*"A model is a simplification or approximation of reality and hence will not reflect all of reality"*

This aphorism summarizes the difficulty of generating statistical models that adequately represent all the nuance & complexity of the real world. At the core of a statistical models success is the quality of data that builds it.

If a data-set inadequately represents the nuance of an area of study, it is unlikely for a statistical model to be able to compensate for this absent variance. This may result in models that are statistically sensible, but otherwise inadequate representations of reality

But how can we know how in touch our model is with reality? Sometimes we will be fortunate that the results of a model can be reconciled against some known external parameter

This was the motivation behind this analysis - to explore & reconcile a statistical model with a known external parameter, and discuss the issues there in. My approach was to use k-means - a technique to explore sub-groups of observations within a data set

The quality of clustering/grouping in k-means can be measured with two types of validity indices 

**Internal indices** - to measure the goodness of a clustering structure without external information (Tseng et al., 2005). In this case, relying upon statistical outputs

**External indices** - to evaluate the results of a clustering algorithm based 
on a known cluster structure of a data set. In other words, already known, real-world groupings

My conduit for this question is a data-set I have curated myself that summarizes average game-day statistics of Australian Rules Football players (i.e. average kicks, handballs, tackles per game)

The goal here is to generate two models - the first optimizing for internal indices  (statistical outputs), the second optimizing for external indices (already known-groupings). 

### Model [1] - Internal Indices

Model 1 aims to find the best combination of statistics,to maximize separation of groupings. Thus, success is defined by internal indicators such as minimizing within-cluster variation, & silhouette plots. Thus, a "good model" is defined strictly by optimizing group-separation

This is important, because in the absence of *any* external indicator, how would we know if the classifications derived from the statistical model are sensible? The statistics may clearly articulate how well the model performs on the data, but not how well the data represents its source-area of inquiry


****** answer - the stats are the best representation of the models data ******

### Model [2] - External Indices 

Of course this can be ameliorated by having an external indicator

AFL players, and sports-people more generally, can be categorized based upon their *field position*. The players position is included in this data, & as such, this model can be evaluated against this external indicator 

Broadly, AFL players fulfill one of four positions:

* Forwards
* Midfielders
* Rucks
* Defenders

Which means our target for this model is four groups (k = 4)

Thus, in contrast to model [1] which we are treating as being agnostic of any external indicator, this model will look to group players into these four field positions. It carries with it the assumption that a players position *is* associated with match-day statistics. This seems intuitive to me... a high average number of *spoils* per game should be associated with players who are defenders.

But as real and true as these field positions may be, it may be that this data doesn't have the sophistication to model them accurately

Which brings us back to the heart of the matter: 

*"A model is a simplification or approximation of reality and hence **will not reflect all of reality**"*

We are trying to condense the complexity & fluidity of an elite sport with a selection of ~ 40 match day statistics. 

So how good can our approximation be? 

If model [1] does not match known field-positions, yet is statistically "superior", how do we interpret it?

Are their statistics absent from this data, that may improve the outputs?

Let's find out

## Some background on clustering 

**"Clustering"** is a technique used to explore sub-groups of observations within a data set. Observations are grouped in such a way that they are more similar to one another (by some statistical criteria), than to observations in other groups

**k-means clustering** is probably the most common algorithm for partitioning 

k-means 'groups' observations (i.e. players) such that the observations within the group are as similar as possible. Clusters are formed via observations that demonstrate high *intra-class similarity*, measured by the clusters *'centroid'* (the mean value of points assigned to the cluster). 

*'Principal components'* are derived as *combinations of the variables* in the data-set

In k-means clustering, 'k' denotes the number of clusters we will separate the data into. This eventually needs to be *pre-specified* to perform the analysis. As mentioned, the # of groups can be estimated statistically (internal indices) or based on known-groupings (external indices). 

For more information on k-means/principal components analysis, [this towards data science](https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff) article is an easily digestible summary 

## The Data

Detailed, match-day statistics are available from two key sources: [AFLTables](https://afltables.com/afl/afl_index.html) & [Footywire](https://www.footywire.com/).

R users are fortunate that the vast depth of statistics from these websites are easily accessible via the **["FitzRoy"](https://cran.r-project.org/web/packages/fitzRoy/index.html)** package

I have covered the process of extracting, cleaning & modeling this data in a **["previous analysis"(https://perkot.github.io/afl-stats/)]**

So for today, Ian Hewitson style, here is data I have prepared earlier

```{r Load dependencies, results='hide', message=FALSE, warning=FALSE}
# Data cleaning 
library(snakecase)
library(tidyr)
library(dplyr)
library(readr)
library(reshape)
library(knitr)

# Data visualisation
library(corrplot)
library(taucharts) # not available for ANZ version of R 
library(purrr) # required for map_dbl
library(GGally)
library(ggplot2)
library(plotly)
library(factoextra)
library(kableExtra)

# Data analysis 
library(ClustOfVar)
library(cluster)
```


```{r data load, results='hide', message=FALSE, warning=FALSE}
# player data .csv 
players <- read.csv("afl_player_statistics.csv", 
                    header = TRUE, 
                    stringsAsFactors = FALSE)
```

```{r print table preview, echo=FALSE}
# Print table 
kable(head(players)) %>% #head limits to just top rows
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", font_size = 8))
```

The data comprises 728 AFL players statistics, averaged across the 2017 & 2020. Each row is a unique player record. Rows 5 through 48 are the statistics we can include in our cluster-analysis 

We have four additional rows summarizing the players name, team, number of games, & importantly, their position

## Pre-processing

There are two important steps I have taken before commencing the cluster analysis 

Firstly, I have removed *any player* to have played under 22 games (equivalent to a single full-season). My reasoning was that players with a very small number of games would skew the results. For example, young players whose inexperience preludes them from generating the typical output of the position they play, or players who got injured resulting in unrepresentative output from a small number of games 

```{r minimum games, message=FALSE, warning=FALSE}
# filter to only players who have played at least an entire season worth of games 
players <- players %>% 
  dplyr::filter(Games >= 22)
```

Secondly, I have *scaled* the data. Scaling is a recommended step for k-means where the inputted variables are on different scales, or at a minimum have different variances. Different scales can be clearly seen in this data (i.e. metres gained measured in metres versus disposal efficiency as a percentage). & eye-balling each metric, variance seems to differ

```{r scale, message=FALSE, warning=FALSE}
# convert statistics to numeric 
players[,5:48] <- sapply(players[,5:48], as.numeric)

# Consider scaled data 
  # center = TRUE : remove the mean
  # scale = TRUE : divide by SD
players_scaled <- scale(players[,5:48], 
                        center = TRUE, 
                        scale = TRUE)
players_scaled <- as.data.frame(players_scaled)
# only numeric 
players_numeric <- dplyr::select_if(players, is.numeric)
```

## Model [1] - internal indices 

```{r scale1, message=FALSE, warning=FALSE}
set.seed(123)
```

```{r Model 1 (1), message=FALSE, warning=FALSE}
Model1 <- players_scaled %>%  
  select(
    # Player_Name,
    # these ruck-related stats distinguish ruckmen entirely from the group
      # 90.6% variance explained with just these stats
    hitouts,
    hitouts_to_advantage,
    ruck_contests,
    score_launches,
    # this midfield related fields produces some separation of midfielders, 
    # reducing overall variance
    # clearances,
    # contested_possessions,
    # uncontested_possessions,
    # centre_clearances,
    # stoppage_clearances,
    # # forwards
    # goals,
    # tackles_inside_fifty,
    # f50_ground_ball_gets,
    # # defenders
    # rebounds,
    #   # one_percenters, keep this out, makes too similar to rucks
    #   # intercepts, # really improves variance, but muddies model
    # contest_def_one_on_ones,
    # disposal_efficiency_percentage
  )
```

```{r model 1 (2), message=FALSE, warning=FALSE}
# fit model into 3 clusters 
KM1 <- kmeans(Model1, 
              centers = 2)
KM1 # 67.1%
```

```{r model 2 (3), message=FALSE, warning=FALSE}
P1 <- fviz_cluster(KM1, 
                   data = Model1) # save to access $data

M1_Data <- P1$data # this is all you need

# calculate the convex hull using chull(), for each cluster
M1_Data_Hull <- M1_Data %>%
  group_by(cluster) %>%
  slice(chull(x, y))

M1_ColourTheme <- c("#E4796A", "#638DCB")
```

```{r model 2 (4), message=FALSE, warning=FALSE}
# plot: you can now customize this by using ggplot sintax
ggplot(M1_Data, aes(x, y)) + 
  geom_point(shape = 1) +
  geom_polygon(data = M1_Data_Hull, 
               alpha = 0.5, 
               aes(fill = cluster, linetype = cluster)) +
  scale_color_manual(values = M1_ColourTheme) +
  scale_fill_manual(values = M1_ColourTheme) +
  
  theme_grey(base_size = 10)+
  theme(legend.position = "right",
        legend.direction = "vertical",
        legend.title = element_text(colour = "#4E4F4E",
                                    size = 8,
                                    face = "bold"),
        # legend.margin = margin(grid::unit(0,"cm")),
        legend.text = element_text(colour = "#4E4F4E",
                                   size = 8),
        legend.key.height = grid::unit(0.6,"cm"),
        legend.key.width = grid::unit(0.6,"cm"),
        legend.margin = margin(0,0,0,0.2,"cm"), # move a little away from plot, to the right
        # axis.text.x = element_blank(),
        # axis.text.y = element_blank(),
        # axis.ticks = element_blank(),
        axis.text.x = element_text(size = 8,
                                   colour = "#4E4F4E"),
        axis.text.y = element_text(size = 8,
                                   vjust = 0.2,
                                   colour = "#4E4F4E"),
        axis.ticks = element_line(size = 0.2, 
                                  colour = "#878683"),
        plot.background = element_rect(fill = "#fcf9f0"),
        panel.background = element_blank(),
        legend.background = element_rect(fill = "#fcf9f0"),
        panel.border = element_blank(),
        axis.line.x = element_line(color = "black"),
        axis.line.y = element_line(color = "black"),
        strip.text.x = element_text(size = 8, colour = "#6b6e6b"),
        strip.background = element_rect(fill="#fffdf2"),
        plot.margin = margin(0.7,0.4,0.1,0.2,"cm"),
        plot.title = element_text(colour = "#4E4F4E",
                                  hjust = 0,
                                  size = 10,
                                  face = "bold"),
        plot.subtitle = element_text(colour = "#6b6e6b",
                                     hjust = 0,
                                     size = 9),
        plot.caption = element_text(colour = "#4E4F4E",
                                    hjust = 0,
                                    vjust = 1,
                                    size = 6,
                                    face = "italic",
                                    margin = margin(-5,0,0,0))) # adjust position ... top, bottom, left, right
```


## Model [2] - external indices 

```{r scale, message=FALSE, warning=FALSE}
set.seed(123)
```

```{r Model 2 (1), message=FALSE, warning=FALSE}
Model_2 <- players_scaled %>%  
  select(
    # Player_Name,
    # these ruck-related stats distinguish ruckmen entirely from the group
      # 90.6% variance explained with just these stats
    hitouts,
    hitouts_to_advantage,
    ruck_contests,
    score_launches,
    # this midfield related fields produces some separation of midfielders, 
    # reducing overall variance
    clearances,
    contested_possessions,
    uncontested_possessions,
    centre_clearances,
    stoppage_clearances,
    # forwards
    goals,
    tackles_inside_fifty,
    f50_ground_ball_gets,
    # defenders
    rebounds,
      # one_percenters, keep this out, makes too similar to rucks
      # intercepts, # really improves variance, but muddies model
    contest_def_one_on_ones,
    disposal_efficiency_percentage
  )
```

```{r model 2 (2), message=FALSE, warning=FALSE}
# fit model into 3 clusters 
K_M2 <- kmeans(Model_2, 
                centers = 4)
K_M2 # 67.1%
```

```{r model 2 (3), message=FALSE, warning=FALSE}
p <- fviz_cluster(K_M2, 
                  data = Model_2) # save to access $data

data <- p$data # this is all you need

# calculate the convex hull using chull(), for each cluster
hull_data <- data %>%
  group_by(cluster) %>%
  slice(chull(x, y))

colour_theme <- c("#E4796A", "#E7B573", "#B0AA96", "#638DCB")
```

```{r model 2 (4), message=FALSE, warning=FALSE}
# plot: you can now customize this by using ggplot sintax
ggplot(data, aes(x, y)) + 
  geom_point(shape = 1) +
  geom_polygon(data = hull_data, 
               alpha = 0.5, 
               aes(fill = cluster, linetype = cluster)) +
  scale_color_manual(values = colour_theme) +
  scale_fill_manual(values = colour_theme) +
  
  theme_grey(base_size = 10)+
  theme(legend.position = "right",
        legend.direction = "vertical",
        legend.title = element_text(colour = "#4E4F4E",
                                    size = 8,
                                    face = "bold"),
        # legend.margin = margin(grid::unit(0,"cm")),
        legend.text = element_text(colour = "#4E4F4E",
                                   size = 8),
        legend.key.height = grid::unit(0.6,"cm"),
        legend.key.width = grid::unit(0.6,"cm"),
        legend.margin = margin(0,0,0,0.2,"cm"), # move a little away from plot, to the right
        # axis.text.x = element_blank(),
        # axis.text.y = element_blank(),
        # axis.ticks = element_blank(),
        axis.text.x = element_text(size = 8,
                                   colour = "#4E4F4E"),
        axis.text.y = element_text(size = 8,
                                   vjust = 0.2,
                                   colour = "#4E4F4E"),
        axis.ticks = element_line(size = 0.2, 
                                  colour = "#878683"),
        plot.background = element_rect(fill = "#fcf9f0"),
        panel.background = element_blank(),
        legend.background = element_rect(fill = "#fcf9f0"),
        panel.border = element_blank(),
        axis.line.x = element_line(color = "black"),
        axis.line.y = element_line(color = "black"),
        strip.text.x = element_text(size = 8, colour = "#6b6e6b"),
        strip.background = element_rect(fill="#fffdf2"),
        plot.margin = margin(0.7,0.4,0.1,0.2,"cm"),
        plot.title = element_text(colour = "#4E4F4E",
                                  hjust = 0,
                                  size = 10,
                                  face = "bold"),
        plot.subtitle = element_text(colour = "#6b6e6b",
                                     hjust = 0,
                                     size = 9),
        plot.caption = element_text(colour = "#4E4F4E",
                                    hjust = 0,
                                    vjust = 1,
                                    size = 6,
                                    face = "italic",
                                    margin = margin(-5,0,0,0))) # adjust position ... top, bottom, left, right
```







Let's read-in FitzRoy, and all other packages we will use for analysis  

```{r Load dependencies, results='hide', message=FALSE, warning=FALSE}
# Data extraction 
library(devtools)
library(fitzRoy)
# Data cleaning 
library(snakecase)
library(tidyr)
library(dplyr)
library(reshape)
library(knitr)
# Data visualisation
library(kableExtra)
library(corrplot)
library(taucharts)
library(kableExtra)
# Data analysis 
library(ClustOfVar)
library(cluster)
```

FitzRoy provides a straight-forward function to read-in data from Footywire (*"get_footywire_stats"*). The only required input is which matches to include in the data-extract via the websites **Match_ids**

The range of "match_ids" has been limited to the 207 games contested in the 2019 season. Match_ids can be identified via the web-URL for each game on FootyWire

I encountered some buggy-behaviour reading in every required match in a single line of code, but found splitting it out worked OK 

The match-data from footywire takes ~ 10-15 minutes to read, so patience is required :)

```{r load data, results='hide', message=FALSE, warning=FALSE}
# Extract all games from 2019 AFL season
footywire2 <- get_footywire_stats(ids = 9876:9927) 
footywire1 <- get_footywire_stats(ids = 9721:9875) 
```

```{r load data2, results='hide', message=FALSE, warning=FALSE}
# Extract all games from 2020 AFL season
footywire3A <- get_footywire_stats(ids = 9928:9936)
footywire3B <- get_footywire_stats(ids = 10126:10141)
footywire3BB <- get_footywire_stats(ids = 10143:10152)
footywire3C <- get_footywire_stats(ids = 10182:10190)
```

``````{r load data456, results='hide', message=FALSE, warning=FALSE}
footywire3D <- get_footywire_stats(ids = 10209:10326) 
```


```{r load data3, results='hide', message=FALSE, warning=FALSE}
# Extract all games from 2018 AFL season
footywire4 <- get_footywire_stats(ids = 9514:9720) 
```

```{r load data4, results='hide', message=FALSE, warning=FALSE}
# Extract all games from 2017 AFL season
footywire5 <- get_footywire_stats(ids = 9307:9513) 
```

```{r bind data, results='hide', message=FALSE, warning=FALSE}
# Bind into a single data-frame 
Season2017to2020 <- rbind(footywire1, footywire2, footywire3A, footywire3B, footywire3BB, footywire3C, footywire3D, footywire4, footywire5)
# lower case, under_score all column titles
names(Season2017to2020) <- to_snake_case(names(Season2017to2020))
# remove original extracts
# footywire1 <- NULL
# footywire2 <- NULL
```

The structure of this data-extract is 207 games x 44 selected players for each game, which produces 9108 rows of data

Each row reports the statistics of an individual player in every game 

This affords great flexibility to explore both match-level and player-level statistics 

```{r print table preview, echo=FALSE}
# Print table 
kable(head(Season2017to2020)) %>% #head limits to just top rows
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", font_size = 8))
```

## Re-shape & aggregate  

Two key steps required to to transform this data for analysis are:

* Aggregate the data so each row reflects match-level statistics rather than        player level, &
* Pivot the data so every row reflects a unique game of the season
  
We will use several "tidyverse" functions to first aggregate our match-level statistics 

```{r change variable types, message=FALSE, warning=FALSE}
# tally up all key statistics per game, per team 
Ag <- Season2017to2020 %>%
  dplyr::select(season, 
         round,
         date,
         team, 
         opposition, 
         status,
         cp, # CONTESTED POSSESSION
         up, # UNCONTESTED POSSESSION 
         de, # DISPOSAL EFFICIENCY 
         one_percenters, # ONE PERCENTERS 
         mg, # METRES GAINED
         to, # TURNOVER 
         k, # KICKS
         hb, # HANDBALLS
         d, # DISPOSALS
         m, # MARK
         i_50, # INSIDE 50
         cl, # CLEARANCE
         cg, # CLANGERS
         r_50, # REBOUND 50 
         ff, # FREES FOR 
         fa, # FREES AGAINST
         cm, # CONTESTED MARKS
         ga_15, # GOAL ASSISTS
         bo, # BOUNCES
         ccl, # CENTRE CLEARANCES
         scl, # STOPPAGE CLEARANCE 
         itc, # INTERCEPTS
         si, # SCORE INVOLVEMENTS 
         t_5, # TACKLES INSIDE 50
         match_id) %>% 
  group_by(match_id, 
           team, 
           opposition, 
           status,
           season, 
           round,
           date) %>%
  summarise(
    CP = sum(cp),
    UP = sum(up),
    DE = round(mean(de),1),
    OP = sum(one_percenters),
    MG = round(sum(mg),1),
    TO = sum(to),
    K = sum(k),
    HB = sum(hb),
    D = sum(d),
    M = sum(m),
    I50 = sum(i_50),
    CL = sum(cl),
    CG = sum(cg),
    R50 = sum(r_50),
    FF = sum(ff),
    FA = sum(fa),
    CM = sum(cm),
    GA = sum(ga_15),
    BO = sum(bo),
    CCL = sum(ccl),
    SCL = sum(scl),
    ITC = sum(itc),
    SI = sum(si),
    T5 = sum(t_5),
  )
```

However, we have one further issue to address: Data for each match is split across two rows - one for the home team, the other for the away team. The required end-state is for data from both teams to be summarized in a single row 

The dplyr function **spread** can *almost* solve this issue. However, it only accepts one "value" argument & we therefore are not able to pivot over multiple variables

Fortunately, clever user [danr from the R Studio Community](https://community.rstudio.com/t/spread-with-multiple-value-columns/5378/2) wrote a function augmenting the spread-command to be able to do this. We will implement this below:

```{r additional date columns }
# remove opposition 
Ag$opposition <- NULL

# Function to spread across multiple values 
myspread <- function(df, key, value) {
  # quote key
  keyq <- rlang::enquo(key)
  # break value vector into quotes
  valueq <- rlang::enquo(value)
  s <- rlang::quos(!!valueq)
  df %>% gather(variable, value, !!!s) %>%
    unite(temp, !!keyq, variable) %>%
    spread(temp, value)
}

# spread - so each row reflects a single game 
Ag <- Ag %>%
  myspread(key = status, value = c(team, CP, UP, DE, OP, MG, TO, K, HB, D, M,
                                   I50, CL, CG, R50, FF, FA, CM, GA, BO, CCL,
                                   SCL, ITC, SI, T5))
```

& after some final tidying ... 

```{r additional date columns2 }
# create common key
Ag$JOIN_ID <- paste(Ag$date, "-", 
                               Ag$Home_team, "-", 
                               Ag$Away_team)

# Re-order Columns
Ag <- Ag[,c(2,3,4,1,55,52,27,30:51,53:54,5:26,28:29)]

# Ensure data-set is actually a dataframe
Ag <- as.data.frame(Ag)
```

We now have an aggregated, pivoted, tidied data-frame. Each game is captured as a unique row, and player statistics have been aggregated into team statistics. 

To be sure the aggregation worked, I spot-checked a number of random matches against official match-day statistics. This confirmed the results of this analysis were consistent with official statistics

However an important feature is missing - **the outcome of the game**. We can't get too far with these statistics if the winning team is unknown

The match-result could in-theory be determined from this data-set. It would require calculating the overall team-scores from individual players goals & behinds

However I decided an easier method would be to source this data directly from [AFL-tables](https://afltables.com/afl/afl_index.html), accessible again from the FitzRoy package  

```{r additional score columns, message = FALSE, warning = FALSE }
# read in data from AFL tables
AT <- get_afltables_stats(start_date = '2017-01-01',
                          end_date = '2020-12-31')

# replace dots with underscores, all lower case
names(AT) <- to_snake_case(names(AT))
```

In order to combine the two data-sets, a common key is required. By combining three columns which exist in **both data-sets** (date, home team & away team), I was able to create my own common key

```{r change variable type }
# create a common key 
AT$JOIN_ID <- paste(AT$date, "-", 
                    AT$home_team, "-", 
                    AT$away_team)

# Select scores - the only rows we would like to keep from this table 
AT <- dplyr::select(AT, 
                  JOIN_ID, 
                  home_score, 
                  away_score)

# remove columns which are not unique 
AT <- AT %>% distinct(JOIN_ID, .keep_all = TRUE)

# Convert to dataframe
AT <- as.data.frame(AT)
```

After some further tidying (error correction & harmonizing team names), the two data-sets are ready to be joined by the calculated **JOINID** column 

```{r merge teams}
# Ensure join columns are comparable 
AT$JOIN_ID <- to_snake_case(AT$JOIN_ID)
Ag$JOIN_ID <- as.character(Ag$JOIN_ID)
Ag$JOIN_ID <- to_snake_case(Ag$JOIN_ID)

# Other corrections
AT$JOIN_ID <- gsub("greater_western_sydney", "gws", AT$JOIN_ID)
AT$JOIN_ID <- gsub("brisbane_lions", "brisbane", AT$JOIN_ID)

# Correct error in AFL tables listing Geelong as home-side in the 2019 Preliminary Final
AT$JOIN_ID <- gsub("2019_09_20_geelong_richmond", "2019_09_20_richmond_geelong", AT$JOIN_ID)

# Join Footywire & AFL tables 
Ag <- left_join(Ag, AT, by = "JOIN_ID")
```

& some final tidying to re-order our variables, remove redundant variables, & convert statistics to numeric format

```{r merge teams 2}
# Re-order columns 
Ag <- Ag[,c(1:7, 56:57, 8:55)]

# Remove IDs - no longer required
Ag$match_id <- NULL
Ag$JOIN_ID <- NULL

# need to change all stats columns from character to numeric 
cols = c(8:55)    
Ag[,cols] = apply(Ag[,cols], 2, function(x) as.numeric(as.character(x)));
```

Our data model is *nearly* complete with match-statistics for both home & away teams, & the total score for each team 

The next step is to calculate the statistical **"differentials"** between the winning & losing teams 

An issue is the winning-score could come from either the home or away score columns. As such a simple subtraction of these fields won't work.

The necessary work-around is to split our data in two: *"winning home team"* & *"winning away team"*, then recombine them into an overall *"winning differentials"* data-frame

Let's start with winning home team:

```{r winning home team}
# "HOME" Winners
# Calculate a "winning margin" score - ultimately we want to see how different
# statistics are related to this outcome variable 
Ag$margin <- Ag$home_score - Ag$away_score

# Subset only "home" team wins 
WinnersHome <- filter(Ag, margin >= 1)

# Create differential columns 
WinnersHome$BO_Diff <- WinnersHome$Home_BO - WinnersHome$Away_BO
WinnersHome$CCL_Diff <- WinnersHome$Home_CCL - WinnersHome$Away_CCL
WinnersHome$CG_Diff <- WinnersHome$Home_CG - WinnersHome$Away_CG
WinnersHome$CL_Diff <- WinnersHome$Home_CL - WinnersHome$Away_CL
WinnersHome$CM_Diff <- WinnersHome$Home_CM - WinnersHome$Away_CM
WinnersHome$CP_Diff <- WinnersHome$Home_CP - WinnersHome$Away_CP
WinnersHome$D_Diff <- WinnersHome$Home_D - WinnersHome$Away_D
WinnersHome$DE_Diff <- WinnersHome$Home_DE - WinnersHome$Away_DE
WinnersHome$BO_Diff <- WinnersHome$Home_BO - WinnersHome$Away_BO
WinnersHome$FA_Diff <- WinnersHome$Home_FA - WinnersHome$Away_FA
WinnersHome$FF_Diff <- WinnersHome$Home_FF - WinnersHome$Away_FF
WinnersHome$GA_Diff <- WinnersHome$Home_GA - WinnersHome$Away_GA
WinnersHome$HB_Diff <- WinnersHome$Home_HB - WinnersHome$Away_HB
WinnersHome$I50_Diff <- WinnersHome$Home_I50 - WinnersHome$Away_I50
WinnersHome$ITC_Diff <- WinnersHome$Home_ITC - WinnersHome$Away_ITC
WinnersHome$K_Diff <- WinnersHome$Home_K - WinnersHome$Away_K
WinnersHome$M_Diff <- WinnersHome$Home_M - WinnersHome$Away_M
WinnersHome$MG_Diff <- WinnersHome$Home_MG - WinnersHome$Away_MG
WinnersHome$OP_Diff <- WinnersHome$Home_OP - WinnersHome$Away_OP
WinnersHome$R50_Diff <- WinnersHome$Home_R50 - WinnersHome$Away_R50
WinnersHome$SCL_Diff <- WinnersHome$Home_SCL - WinnersHome$Away_SCL
WinnersHome$SI_Diff <- WinnersHome$Home_SI - WinnersHome$Away_SI
WinnersHome$TO_Diff <- WinnersHome$Home_TO - WinnersHome$Away_TO
WinnersHome$UP_Diff <- WinnersHome$Home_UP - WinnersHome$Away_UP
WinnersHome$T5_Diff <- WinnersHome$Home_T5 - WinnersHome$Away_T5

# Designate these games as "home" team winning 
WinnersHome$Winner <- "HOME"
WinnersHome$WinningTeam <- WinnersHome$Home_team

cols = c(1:7,82,56:81) 
WinnersHome <- WinnersHome[, cols]
```

Repeat this step for "winning away team"

```{r winning away team}
# "AWAY" Winners
# Subset only "away" team wins 
WinnersAway <- filter(Ag, margin <= -1)

# Create differential columns 
WinnersAway$BO_Diff <- WinnersAway$Away_BO - WinnersAway$Home_BO
WinnersAway$CCL_Diff <- WinnersAway$Away_CCL - WinnersAway$Home_CCL
WinnersAway$CG_Diff <- WinnersAway$Away_CG - WinnersAway$Home_CG
WinnersAway$CL_Diff <- WinnersAway$Away_CL - WinnersAway$Home_CL
WinnersAway$CM_Diff <- WinnersAway$Away_CM - WinnersAway$Home_CM
WinnersAway$CP_Diff <- WinnersAway$Away_CP - WinnersAway$Home_CP
WinnersAway$D_Diff <- WinnersAway$Away_D - WinnersAway$Home_D
WinnersAway$DE_Diff <- WinnersAway$Away_DE - WinnersAway$Home_DE
WinnersAway$BO_Diff <- WinnersAway$Away_BO - WinnersAway$Home_BO
WinnersAway$FA_Diff <- WinnersAway$Away_FA - WinnersAway$Home_FA
WinnersAway$FF_Diff <- WinnersAway$Away_FF - WinnersAway$Home_FF
WinnersAway$GA_Diff <- WinnersAway$Away_GA - WinnersAway$Home_GA
WinnersAway$HB_Diff <- WinnersAway$Away_HB - WinnersAway$Home_HB
WinnersAway$I50_Diff <- WinnersAway$Away_I50 - WinnersAway$Home_I50
WinnersAway$ITC_Diff <- WinnersAway$Away_ITC - WinnersAway$Home_ITC
WinnersAway$K_Diff <- WinnersAway$Away_K - WinnersAway$Home_K
WinnersAway$M_Diff <- WinnersAway$Away_M - WinnersAway$Home_M
WinnersAway$MG_Diff <- WinnersAway$Away_MG - WinnersAway$Home_MG
WinnersAway$OP_Diff <- WinnersAway$Away_OP - WinnersAway$Home_OP
WinnersAway$R50_Diff <- WinnersAway$Away_R50 - WinnersAway$Home_R50
WinnersAway$SCL_Diff <- WinnersAway$Away_SCL - WinnersAway$Home_SCL
WinnersAway$SI_Diff <- WinnersAway$Away_SI - WinnersAway$Home_SI
WinnersAway$TO_Diff <- WinnersAway$Away_TO - WinnersAway$Home_TO
WinnersAway$UP_Diff <- WinnersAway$Away_UP - WinnersAway$Home_UP
WinnersAway$T5_Diff <- WinnersAway$Away_T5 - WinnersAway$Home_T5

# Designate these games as "home" team winning 
WinnersAway$Winner <- "AWAY"

# Designate these games as "home" team winning 
WinnersAway$WinningTeam <- WinnersAway$Away_team

cols = c(1:7,82,56:81) 
WinnersAway <- WinnersAway[, cols]

# Change margin to absolute
WinnersAway$margin <- abs(WinnersAway$margin)
```

Now re-combine these two data-sets into our final data-frame of "winning differentials"

```{r bind home and away}
# Bind together 
Matches <- rbind(WinnersHome, WinnersAway)

# Alternative version with only numeric columns 
cols = c(9:33)
Matches_Numeric <- Matches[, cols]
```

Which results in the final data-model 

Each row reflects a single match of the 2019 season, & includes: 

* the teams involved in each match
* the date of the match
* which team won
* the margin of the win
* 24 "statistical differences" for the winning team 

```{r print table preview3, echo=FALSE}
kable(head(Matches)) %>% #head limits to just top rows
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", font_size = 8))
```

## Prioritise Statistics of Interest

It's worth taking a moment to review the included statistics. Many metrics are very similar, so we would expect collinearity between them. Based on little more than intuition, I have opted to delete the below metrics: 

* Removed "Frees-Against", as it is a perfect mirror of "Frees-For" - differences here are only really useful at an individual player level rather than in aggregate
* Removed "centre clearances" & "stoppage clearances" in favor of keeping "total clearances" for this analysis 
* Removed "total disposals" in favor of keeping its constituent elements - total kicks & handballs 
* Removed "total marks" in favor of keeping total contested marks (a subset of total marks). I figured contested marking would be more closely associated with winning margin
* Removed "turn-overs" in favor of keeping intercept possessions as these two measures are unsurprisingly highly correlated
* Removed "Score Involvements" & "Goal Assists". These metrics are clearly a function of the score margin, & as such would be uninformative to any analysis

```{r trim data}
Matches_Numeric$FA_Diff <- NULL # FA a mirror of FF 
Matches_Numeric$CCL_Diff <- NULL # already have clearance data
Matches_Numeric$SCL_Diff <- NULL # already have clearance data
Matches_Numeric$D_Diff <- NULL # already have kick & handball data
Matches_Numeric$M_Diff <- NULL # selected contested marking instead
Matches_Numeric$TO_Diff <- NULL # selected contested marking instead
Matches_Numeric$SI_Diff <- NULL # in effect, a component of margin
Matches_Numeric$GA_Diff <- NULL # in effect, a component of margin
```

## Visualise & Explore 

### Correlation Matrices 

The R package [Corrplot](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) is one of the easiest, & most engaging ways to summarize the relationships between variables via a matrix of correlations 

Let's first create a matrix of correlations

```{r cor matrix}
# create matrix of correlations
M <- cor(Matches_Numeric)
# round data to 2 decimal places
M <- round(M, 2)
```

& now generate two correlation plots for the data 

The *first* plot uses coloured squares to summarize the correlations between 
variables, ranging from  navy (perfect positive correlation) to maroon (perfect negative correlation). The strength of the correlation is also depicted by the size of the square

The *second* plot summarises the same information, but provides the actual R values instead of colored squares 

With corrplot you can pass arguments to augment aesthetics such as text-size. These text-editing steps are important to improve the aesthetic of the plot(s), particularly when there are many variables, and/or long variable names 

```{r cor matrix 2}
CM1 <- 
corrplot(M,
         method = "square", 
         type = "upper",
         tl.col= "black", 
         tl.cex = 0.6, # Text label color and rotation
         cl.cex = 0.6 # Correlation label color and rotation
         )

CM2 <- 
corrplot(M,
         method = "number", 
         type = "upper",
         tl.col="black", 
         # tl.srt=45, 
         tl.cex = 0.6, # Text label color and rotation
         cl.cex = 0.6, # Correlation label color and rotation
         number.cex = .6
         )
```

In the above Figures, the **top horizontal row of the grid** is of most interest. This row depicts each *statistical-differentials* correlation with the *winning margin* 

The immediate stand-out along this row is MG_Diff : **Metres Gained Differential**

What this part of the grid shows is the larger the number of metres gained by the winning team, the greater the winning margin. This is far-and-away the largest correlation of all statistics, which we can characterize as positive & strong **(R = 0.82)**. 

Metres Gained has become one of the most fashionable statistics in AFL. An in-depth examination of what it means & does not mean can be read [here](https://www.espn.com.au/afl/story/_/id/27199381/metres-gained-breaking-afl-most-misunderstood-statistic). 

It should be acknowledged the gurus at Champion Data have delved into this statistic in much greater depth. My characterization here is simplistic. Much of the nuance which is absent here is covered in [this article](https://www.foxsports.com.au/afl/afl-2020-afl-finals-afl-grand-final-premiership-odds-predictor-champion-data-premiership-profile-2020/news-story/d1ce45ef3111bb82cad8d2157664b922?fbclid=IwAR3tKoj_bVRbI3khWzZAqJ3E79ugjNIpe5_kkl2Zcr4vLbWUihA2tnGOhNY) (for example, delineating *effective* metres gained)

Comparatively, the next most important metrics were moderate in strength. These were effective disposal percentage differential of the winning side **(R = 0.47)**, and how many more kicks the winning side had **(R=0.42)**. 

We can look more closely at these relationships via a series of interactive scatterplots:

### Scatterplots

#### winning-Margin x Metres-Gained-Differential

```{r cor plot 1}
# the greater the MG discrepancy 
Matches %>% 
  dplyr::select(Home_team, Away_team, WinningTeam, margin, MG_Diff) %>% 
  tauchart() %>% 
  tau_point("margin", "MG_Diff") %>% 
  tau_tooltip() 
```

Examining the scatterplot of metres gained & winning margin, a few further insights can be gleaned:

* there are very few games where the winning team loses the metres-gained differential **(only 25 out of 207)**
* The highest winning margin for the year for a team conceding the metres-gained statistic was 24 points (Geelong defeating North Melbourne, with a very small -9 metres gained). In other words, if a team does not win the metres gained statistic, they are very unlikely to win by a large margin 
* The largest amount of metres gained conceded for a winning team was *-461 metres*, with Fremantle defeating Sydney despite clearly losing the territory battle 
* Fremantle were also involved in the only clear outlying game, when they lost to West Coast by 91 points, but conceded only 559 metres (an abnormally small difference when compared to other losses of that margin)

#### Winning-Margin x Disposal-Efficiency-Differential

```{r cor plot 2}
Matches %>% 
  dplyr::select(Home_team, Away_team, WinningTeam, margin, DE_Diff) %>% 
  tauchart() %>% 
  tau_point("margin", "DE_Diff") %>% 
  tau_tooltip() 
```

One interesting observation between winning margin and disposal efficiency differential is the poorest differential observed for a winning team was -8.7%, recorded by Collingwood in a one point defeat of West-Coast

#### winning-Margin x Number-of-Kicks-Differential

```{r cor plot 3}
# the greater the MG discrepancy 
Matches %>% 
  dplyr::select(Home_team, Away_team, WinningTeam, margin, K_Diff) %>% 
  tauchart() %>% 
  tau_point("margin", "K_Diff") %>% 
  tau_tooltip() 
```

There are four potential outlier matches in this plot, with all involving Richmond. In three of these outliers, Richmond won comfortably, despite recording more than 40 less kicks than there opposition (wins against Sydney, St Kilda & Brisbane). 

Inversely, Richmond were defeated by Collingwood in round 2 by 44 points, conceding the equal largest amount of kicks for any game (+107). Other matches with similar kick-differentials resulted in much greater winning margins 